{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nech-C/comp5300-project/blob/main/5530project_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "import lib, mount google drive, clone comp5300, etc.\n",
        "\n"
      ],
      "metadata": {
        "id": "uDMKzR9-NmDR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p34CaXtZJeH8"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/My Drive/GQA/\"\n",
        "# import lib\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import shutil\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')\n",
        "# Replace 'My Drive' with the specific path where you want to save the files in your Drive if needed\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "%cd \"$path\"\n",
        "\n",
        "!git -C \"/content/drive/My Drive/GQA/comp5300-project\" pull\n",
        "\n",
        "sys.path.append(os.path.join(path, \"comp5300-project\"))\n",
        "from utils.dataset_processing import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5JlwI3_I5Ya",
        "outputId": "2b74eecd-5647-4354-b0a8-3b160417820f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot create regular file '/content/gqa/images': No such file or directory\n",
            "cp: cannot create regular file '/content/gqa/questions': No such file or directory\n",
            "cp: cannot create regular file '/content/gqa/sceneGraphs': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# constants - google drive\n",
        "SCRIPT_PATH = \"/content/drive/MyDrive/GQA/comp5300-project\"\n",
        "IMAGE_ZIP = '/content/drive/MyDrive/GQA/images.zip'\n",
        "QUESTIONS_ZIP = '/content/drive/MyDrive/GQA/questions.zip'\n",
        "SCENE_GRAPHS_ZIP = '/content/drive/MyDrive/GQA/sceneGraphs.zip'\n",
        "\n",
        "# constants - vm\n",
        "BASE = '/content/gqa'\n",
        "\n",
        "!cp $IMAGE_ZIP $BASE/images\n",
        "!cp $QUESTIONS_ZIP $BASE/questions\n",
        "!cp $SCENE_GRAPHS_ZIP $BASE/sceneGraphs\n",
        "# create dirs and unzip those in BASE\n",
        "!mkdir -p $BASE/images\n",
        "!mkdir -p $BASE/questions\n",
        "!mkdir -p $BASE/sceneGraphs\n",
        "!unzip -q $QUESTIONS_ZIP -d $BASE/questions\n",
        "!unzip -q $SCENE_GRAPHS_ZIP -d $BASE/sceneGraphs\n",
        "!unzip -q $IMAGE_ZIP -d $BASE/images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zh__RZgcBY-G",
        "outputId": "e721b5b3-dcc3-4e13-8f56-aeebabf7062b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing completed.\n"
          ]
        }
      ],
      "source": [
        "# preprocess questions\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "TRAIN_ALL_QUESTIONS = \"/content/gqa/questions/train_all_questions\"\n",
        "IMAGES = \"/content/gqa/images/images\"\n",
        "PROCESSED_QUESTIONS = \"/content/drive/MyDrive/GQA/processed_questions\"\n",
        "\n",
        "# Create the processed_questions directory if it doesn't exist\n",
        "os.makedirs(PROCESSED_QUESTIONS, exist_ok=True)\n",
        "\n",
        "# Get a list of all the image names in the images directory\n",
        "image_names = set(os.listdir(IMAGES))\n",
        "\n",
        "# Get a list of all the JSON files in the train_all_questions directory\n",
        "json_files = [f for f in os.listdir(TRAIN_ALL_QUESTIONS) if f.endswith('.json')]\n",
        "\n",
        "for json_file in json_files:\n",
        "    json_path = os.path.join(TRAIN_ALL_QUESTIONS, json_file)\n",
        "\n",
        "    with open(json_path, 'r') as f:\n",
        "        questions_data = json.load(f)\n",
        "\n",
        "    processed_questions = {}\n",
        "\n",
        "    for question_id, question_info in questions_data.items():\n",
        "        image_id = question_info['imageId']\n",
        "        image_name = image_id + '.jpg'\n",
        "\n",
        "        # Check if the image exists in the set of image names\n",
        "        if image_name in image_names:\n",
        "            processed_question = {\n",
        "                'imageId': image_id,\n",
        "                'question': question_info['question'],\n",
        "                'answer': question_info['answer'],\n",
        "                'fullAnswer': question_info['fullAnswer'],\n",
        "                'groups': question_info['groups']\n",
        "            }\n",
        "            processed_questions[question_id] = processed_question\n",
        "    # Save the processed questions to a new JSON file\n",
        "    processed_json_path = os.path.join(PROCESSED_QUESTIONS, json_file)\n",
        "    with open(processed_json_path, 'w') as f:\n",
        "        json.dump(processed_questions, f, indent=2)\n",
        "\n",
        "print(\"Processing completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create trainable .jsons\n",
        "TRAIN_ALL_QUESTIONS = \"/content/gqa/questions/train_all_questions\"\n",
        "IMAGES = \"/content/gqa/images/images\"\n",
        "PROCESSED_QUESTIONS = \"/content/drive/MyDrive/GQA/processed_questions\"\n",
        "processed_json_files = [os.path.join(PROCESSED_QUESTIONS, f) for f in os.listdir(PROCESSED_QUESTIONS) if f.endswith('.json')]\n",
        "trainable_save_path = \"/content/drive/MyDrive/GQA/trainable_questions\"\n",
        "os.makedirs(trainable_save_path, exist_ok=True)\n",
        "\n",
        "for json_file in processed_json_files:\n",
        "    with open(json_file, 'r') as f:\n",
        "        questions_data = json.load(f)\n",
        "\n",
        "        # generate trainable .json\n",
        "        trainable_questions = []\n",
        "        for question_id, question_info in questions_data.items():\n",
        "            image_name = question_info['imageId'] + '.jpg'\n",
        "\n",
        "            trainable_question = {\n",
        "              'id': question_id,\n",
        "              'image': image_name,\n",
        "              \"conversations\": [\n",
        "                  {\n",
        "                      \"from\": \"human\",\n",
        "                      \"value\": question_info['question']\n",
        "                  },\n",
        "                  {\n",
        "                      \"from\": \"gpt\",\n",
        "                      \"value\": question_info['answer']\n",
        "                  }\n",
        "              ],\n",
        "            }\n",
        "            trainable_questions.append(trainable_question)\n",
        "            # end of for\n",
        "\n",
        "        trainable_json_path = os.path.join(\n",
        "            trainable_save_path,\n",
        "            os.path.splitext(os.path.basename(json_file))[0] + \"_trainable.json\")\n",
        "\n",
        "        with open(trainable_json_path, 'w') as f:\n",
        "            json.dump(trainable_questions, f, indent=2)\n",
        "\n",
        "print(\"Processing completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3h_oeMkc8Azv",
        "outputId": "e70e0a54-cd94-4609-9ac1-fe3f361e8220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NEW START POINT"
      ],
      "metadata": {
        "id": "BXNwvyh-alCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Ot2yvV4Zgr_i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52a05a22-d18f-4e32-abbb-7fa1ea264749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# copy trainables to disk\n",
        "!pip install deepspeed\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/liuhaotian/llava-v1.6-mistral-7b\n",
        "!git clone https://github.com/bdytx5/finetune_LLaVA"
      ],
      "metadata": {
        "id": "Wett2Qo_voIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_ZIP = '/content/drive/MyDrive/GQA/images.zip'\n",
        "QUESTIONS_ZIP = '/content/drive/MyDrive/GQA/questions.zip'\n",
        "TRAINABLE_PATH = '/content/drive/MyDrive/GQA/trainable_questions'\n",
        "\n",
        "# constants - vm\n",
        "BASE = '/content/datasets'\n",
        "\n",
        "!cp $IMAGE_ZIP /content\n",
        "# create dirs and unzip those in BASE\n",
        "!mkdir -p $BASE/images\n",
        "!unzip -q /content/images.zip -d $BASE/images\n",
        "!cp -r $TRAINABLE_PATH $BASE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVJNoNjnZT0v",
        "outputId": "48d41e8c-c9cf-402c-aa8b-caff4c950817"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot create regular file '/content/datasets/images': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "%cd /content/finetune_LLaVA\n",
        "!pip install -e .\n",
        "!pip install -e \".[train]\"\n",
        "!pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "id": "4WXLupipIr5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "with open('/content/datasets/trainable_questions/train_all_questions_0_trainable.json', 'r') as f:\n",
        "  data = json.load(f)\n",
        "  # get random 6000 items and save to /content/datasets/trainable_questions/train_questions_trainable.json\n",
        "  random_items = random.sample(data, 10000)\n",
        "  with open('/content/datasets/trainable_questions/train_questions_trainable.json', 'w') as f:\n",
        "    json.dump(random_items, f, indent=2)"
      ],
      "metadata": {
        "id": "1Z5zRdxe1oWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/datasets/trainable_questions/train_all_questions_1_trainable.json', 'r') as f:\n",
        "  data = json.load(f)\n",
        "  # get random 3000 items save to /content/datasets/trainable_questions/val_questions.json\n",
        "  random_items = random.sample(data, 3000)\n",
        "  with open('/content/datasets/trainable_questions/val_questions.json', 'w') as f:\n",
        "    json.dump(random_items, f, indent=2)"
      ],
      "metadata": {
        "id": "tLEbo87W6Htc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append('/content/LLaVA/llava')\n",
        "sys.path.append('/content/LLaVA')\n",
        "!deepspeed /content/finetune_LLaVA/llava/train/train_mem.py \\\n",
        "--deepspeed /content/finetune_LLaVA/scripts/zero2.json \\\n",
        "--lora_enable True \\\n",
        "--lora_r 128 \\\n",
        "--lora_alpha 256 \\\n",
        "--mm_projector_lr 2e-5 \\\n",
        "--bits 4 \\\n",
        "--model_name_or_path /content/llava-v1.6-mistral-7b \\\n",
        "--version llava_llama_1.6 \\\n",
        "--data_path /content/datasets/trainable_questions/train_questions_trainable.json \\\n",
        "--validation_data_path /content/datasets/trainable_questions/val_questions.json \\\n",
        "--image_folder /content/datasets/images/images\\\n",
        "--vision_tower openai/clip-vit-large-patch14-336 \\\n",
        "--mm_projector_type mlp2x_gelu \\\n",
        "--mm_vision_select_layer -2 \\\n",
        "--mm_use_im_start_end False \\\n",
        "--mm_use_im_patch_token False \\\n",
        "--image_aspect_ratio pad \\\n",
        "--group_by_modality_length True \\\n",
        "--bf16 True \\\n",
        "--output_dir /content/llama-1.6-7b-chat-task-qlora \\\n",
        "--num_train_epochs 20 \\\n",
        "--per_device_train_batch_size 256 \\\n",
        "--per_device_eval_batch_size 256 \\\n",
        "--gradient_accumulation_steps 1 \\\n",
        "--evaluation_strategy \"epoch\" \\\n",
        "--save_strategy \"steps\" \\\n",
        "--save_steps 500 \\\n",
        "--save_total_limit 1 \\\n",
        "--learning_rate 2e-4 \\\n",
        "--weight_decay 0. \\\n",
        "--warmup_ratio 0.03 \\\n",
        "--lr_scheduler_type \"cosine\" \\\n",
        "--logging_steps 1 \\\n",
        "--tf32 True \\\n",
        "--model_max_length 2048 \\\n",
        "--gradient_checkpointing True \\\n",
        "--dataloader_num_workers 2 \\\n",
        "--lazy_preprocess True \\\n",
        "--report_to wandb"
      ],
      "metadata": {
        "id": "j8CQ7KXj0zh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluate fine tuned model:"
      ],
      "metadata": {
        "id": "iNPYUGa1Krxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clone required repos\n",
        "!pip install deepspeed\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/liuhaotian/llava-v1.6-mistral-7b\n",
        "!git clone https://github.com/bdytx5/finetune_LLaVA\n",
        "\n",
        "# instal dependecies\n",
        "!pip install --upgrade pip\n",
        "%cd /content/finetune_LLaVA\n",
        "!pip install -e .\n",
        "!pip install -e \".[train]\"\n",
        "!pip install flash-attn --no-build-isolation\n",
        "\n",
        "# copy files"
      ],
      "metadata": {
        "id": "3VnODb_FN_Q_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyN3uxeP+CxskLbdIjdKVh5V",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}